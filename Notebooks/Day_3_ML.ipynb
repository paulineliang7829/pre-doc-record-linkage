{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"../images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan. \n",
    "\n",
    "_Citation to be updated on notebook export_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-1\">Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\">Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Glossary-of-Terms\" data-toc-modified-id=\"Glossary-of-Terms-1.1.1\">Glossary of Terms</a></span></li></ul></li><li><span><a href=\"#Python-Setup\" data-toc-modified-id=\"Python-Setup-1.2\">Python Setup</a></span></li><li><span><a href=\"#The-Machine-Learning-Process\" data-toc-modified-id=\"The-Machine-Learning-Process-1.3\">The Machine Learning Process</a></span></li><li><span><a href=\"#Problem-Formulation\" data-toc-modified-id=\"Problem-Formulation-1.4\">Problem Formulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Four-Main-Types-of-ML-Tasks-for-Policy-Problems\" data-toc-modified-id=\"Four-Main-Types-of-ML-Tasks-for-Policy-Problems-1.4.1\">Four Main Types of ML Tasks for Policy Problems</a></span></li><li><span><a href=\"#Our-Machine-Learning-Problem\" data-toc-modified-id=\"Our-Machine-Learning-Problem-1.4.2\">Our Machine Learning Problem</a></span></li></ul></li><li><span><a href=\"#Data-Exploration-and-Preparation\" data-toc-modified-id=\"Data-Exploration-and-Preparation-1.5\">Data Exploration and Preparation</a></span></li><li><span><a href=\"#Building-a-Model-and-Model-Fitting\" data-toc-modified-id=\"Building-a-Model-and-Model-Fitting-1.6\">Building a Model and Model Fitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-and-Test-Sets\" data-toc-modified-id=\"Training-and-Test-Sets-1.6.1\">Training and Test Sets</a></span></li><li><span><a href=\"#Data-Distribution\" data-toc-modified-id=\"Data-Distribution-1.6.2\">Data Distribution</a></span></li><li><span><a href=\"#Crosstabs\" data-toc-modified-id=\"Crosstabs-1.6.3\">Crosstabs</a></span></li><li><span><a href=\"#Selecting-Predictors/Features-and-what-we-are-predicting-(Labels)\" data-toc-modified-id=\"Selecting-Predictors/Features-and-what-we-are-predicting-(Labels)-1.6.4\">Selecting Predictors/Features and what we are predicting (Labels)</a></span></li><li><span><a href=\"#Creating-dummy-variables\" data-toc-modified-id=\"Creating-dummy-variables-1.6.5\">Creating dummy variables</a></span></li><li><span><a href=\"#handling-missing-values\" data-toc-modified-id=\"handling-missing-values-1.6.6\">handling missing values</a></span></li><li><span><a href=\"#Scaling-values\" data-toc-modified-id=\"Scaling-values-1.6.7\">Scaling values</a></span></li></ul></li><li><span><a href=\"#Model-Understanding-and-Evaluation\" data-toc-modified-id=\"Model-Understanding-and-Evaluation-1.7\">Model Understanding and Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Running-a-Machine-Learning-Model\" data-toc-modified-id=\"Running-a-Machine-Learning-Model-1.7.1\">Running a Machine Learning Model</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-1.7.2\">Model Evaluation</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-1.7.3\">Confusion Matrix</a></span></li><li><span><a href=\"#Evaluation-metrics\" data-toc-modified-id=\"Evaluation-metrics-1.7.4\">Evaluation metrics</a></span></li><li><span><a href=\"#Precision-and-Recall-at-k%\" data-toc-modified-id=\"Precision-and-Recall-at-k%-1.7.5\">Precision and Recall at k%</a></span></li><li><span><a href=\"#Feature-Understanding\" data-toc-modified-id=\"Feature-Understanding-1.7.6\">Feature Understanding</a></span></li></ul></li><li><span><a href=\"#Decision-tree-model\" data-toc-modified-id=\"Decision-tree-model-1.8\">Decision tree model</a></span></li><li><span><a href=\"#Assessing-Model-Against-Baselines\" data-toc-modified-id=\"Assessing-Model-Against-Baselines-1.9\">Assessing Model Against Baselines</a></span></li><li><span><a href=\"#Machine-Learning-Pipeline\" data-toc-modified-id=\"Machine-Learning-Pipeline-1.10\">Machine Learning Pipeline</a></span></li><li><span><a href=\"#Survey-of-Algorithms\" data-toc-modified-id=\"Survey-of-Algorithms-1.11\">Survey of Algorithms</a></span></li><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-1.12\">Exercise</a></span></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-1.13\">Additional Resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this tutorial, we'll discuss how to formulate a research question in the machine learning framework; how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `scikit-learn` package in Python to model the data you're familiar with from previous tutorials. \n",
    "\n",
    "\n",
    "This tutorial is based on chapter 6 of [Big Data and Social Science](https://github.com/BigDataSocialScience/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary of Terms\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "There are a number of terms specific to machine learning that you will find repeatedly in this notebook. \n",
    "\n",
    "- **Learning**: In Machine Learning, you'll hear about \"learning a model.\" This is what you probably know as \n",
    "*fitting* or *estimating* a function, or *training* or *building* a model. These terms are all synonyms and are \n",
    "used interchangeably in machine learning literature.\n",
    "- **Examples**: These are what you probably know as *data points* or *observations* or *rows*. \n",
    "- **Features**: These are what you probably know as *independent variables*, *attributes*, *predictors*, \n",
    "or *explanatory variables.*\n",
    "- **Underfitting**: This happens when a model is too simple and does not capture the structure of the data well \n",
    "enough.\n",
    "- **Overfitting**: This happens when a model is too complex or too sensitive to the noise in the data; this can\n",
    "result in poor generalization performance or applicability of the model to new data. \n",
    "- **Regularization**: This is a general method to avoid overfitting by applying additional constraints to the model. \n",
    "For example, you can limit the number of features present in the final model, or the weight coefficients applied\n",
    "to the (standardized) features are small.\n",
    "- **Supervised learning** involves problems with one target or outcome variable (continuous or discrete) that we want\n",
    "to predict, or classify data into. Classification, prediction, and regression fall into this category. We call the\n",
    "set of explanatory variables $X$ **features**, and the outcome variable of interest $Y$ the **label**.\n",
    "- **Unsupervised learning** involves problems that do not have a specific outcome variable of interest, but rather\n",
    "we are looking to understand \"natural\" patterns or groupings in the data - looking to uncover some structure that \n",
    "we do not know about a priori. Clustering is the most common example of unsupervised learning, and another example is \n",
    "principal components analysis (PCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `psycopg2` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to fit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "# from __future__ import division \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":1.25, \"lines.markersize\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The machine learning Process is as follows:\n",
    "\n",
    "- [**Understand the problem and goal.**](#problem-formulation) *This sounds obvious but is often nontrivial.* Problems typically start as vague \n",
    "descriptions of a goal - improving health outcomes, increasing graduation rates, understanding the effect of a \n",
    "variable *X* on an outcome *Y*, etc. It is really important to work with people who understand the domain being\n",
    "studied to dig deeper and define the problem more concretely. What is the analytical formulation of the metric \n",
    "that you are trying to optimize?\n",
    "- [**Formulate it as a machine learning problem.**](#problem-formulation) Is it a classification problem or a regression problem? Is the \n",
    "goal to build a model that generates a ranked list prioritized by risk, or is it to detect anomalies as new data \n",
    "come in? Knowing what kinds of tasks machine learning can solve will allow you to map the problem you are working on\n",
    "to one or more machine learning settings and give you access to a suite of methods.\n",
    "- **Data exploration and preparation.** Next, you need to carefully explore the data you have. What additional data\n",
    "do you need or have access to? What variable will you use to match records for integrating different data sources?\n",
    "What variables exist in the data set? Are they continuous or categorical? What about missing values? Can you use the \n",
    "variables in their original form, or do you need to alter them in some way?\n",
    "- [**Feature engineering.**](#feature-generation) In machine learning language, what you might know as independent variables or predictors \n",
    "or factors or covariates are called \"features.\" Creating good features is probably the most important step in the \n",
    "machine learning process. This involves doing transformations, creating interaction terms, or aggregating over data\n",
    "points or over time and space.\n",
    "- **Method selection.** Having formulated the problem and created your features, you now have a suite of methods to\n",
    "choose from. It would be great if there were a single method that always worked best for a specific type of problem. Typically, in machine learning, you take a variety of methods and try them, empirically validating which one is the best approach to your problem.\n",
    "- [**Evaluation.**](#evaluation) As you build a large number of possible models, you need a way choose the best among them. We'll cover methodology to validate models on historical data and discuss a variety of evaluation metrics. The next step is to validate using a field trial or experiment.\n",
    "- [**Deployment.**](#deployment) Once you have selected the best model and validated it using historical data as well as a field\n",
    "trial, you are ready to put the model into practice. You still have to keep in mind that new data will be coming in,\n",
    "and the model might change over time.\n",
    "\n",
    "\n",
    "\n",
    "You're probably used to fitting models in physical or social science classes. In those cases, you probably had a hypothesis or theory about the underlying process that gave rise to your data, chose an appropriate model based on prior knowledge and fit it using least squares, and used the resulting parameter or coefficient estimates (or confidence intervals) for inference. This type of modeling is very useful for *interpretation*.\n",
    "\n",
    "In machine learning, our primary concern is *generalization*. This means that:\n",
    "- **We care less about the structure of the model and more about the performance** This means that we'll try out a whole bunch of models at a time and choose the one that works best rather than determining which model to use ahead of time. We can then choose to select a *suboptimal* model if we care about a specific model type. \n",
    "- **We don't (necessarily) want the model that best fits the data we've *already seen*,** but rather the model that will perform the best on *new data*. This means that we won't gauge our model's performance using the same data that we used to fit the model (e.g., sum of squared errors or $R^2$), and that \"best fit\" or accuracy will most often *not* determine the best model.  \n",
    "- **We can include a lot of variables in to the model.** This may sound like the complete opposite of what you've heard in the past, and it can be hard to swallow. But we will use different methods to deal with many of those concerns in the model fitting process by using a more automatic variable selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, turning something into a real objective function. What do you care about? Do you have data on that thing? What action can you take based on your findings? Do you risk introducing any bias based on the way you model something? \n",
    "\n",
    "### Four Main Types of ML Tasks for Policy Problems\n",
    "\n",
    "- **Description**: [How can we identify and respond to the most urgent online government petitions?](https://dssg.uchicago.edu/project/improving-government-response-to-citizen-requests-online/)\n",
    "- **Prediction**: [Which students will struggle academically by third grade?](https://dssg.uchicago.edu/project/predicting-students-that-will-struggle-academically-by-third-grade/)\n",
    "- **Detection**: [Which police officers are likely to have an adverse interaction with the public?](https://dssg.uchicago.edu/project/expanding-our-early-intervention-system-for-adverse-police-interactions/)\n",
    "- **Behavior Change**: [How can we prevent juveniles from interacting with the criminal justice system?](https://dssg.uchicago.edu/project/preventing-juvenile-interactions-with-the-criminal-justice-system/)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The notebook used to create the sample data is [ML_data_mash.ipynb](ML_data_mash.ipynb) \n",
    "\n",
    "Reference information to keep in mind for Machine Learning prediction models.\n",
    "1. **Creating labels**: Labels are the dependent variables, or Y variables, that we are trying to predict. \n",
    "\n",
    "1. **Decide on feature**: Our features are our independent variables or predictors. Good features make machine learning systems effective. The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. Machine learning algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem.\n",
    "\n",
    "1. **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text. Example of feature engineering are: \n",
    "    - **Transformations**, such a log, square, and square root.\n",
    "    - **Dummy (binary) variables**, sometimes known as *indicator variables*, often done by taking categorical variables (such as industry) which do not have a numeric value, and adding them to models as a binary value.\n",
    "    - **Discretization**. Several methods require features to be discrete instead of continuous. This is often done by binning, which you can do by equal width, deciles, Fisher-Jenks, etc. \n",
    "    - **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several values into one feature, aggregating over varying windows of time and space. For example, we may want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    "1. **Cleaning data**: To run the `scikit-learn` set of models we demonstrate in this notebook, your input dataset must have no missing variables.\n",
    "\n",
    "1. **Imputing values to missing or irrelevant data**: Once the features are created, always check to make sure the values make sense. You might have some missing values, or impossible values for a given variable (negative values, major outliers). If you have missing values you should think hard about what makes the most sense for your problem; you may want to replace with `0`, the median or mean of your data, or some other value.\n",
    "\n",
    "1. **Scaling features**: Certain models will have an issue with features on different scales. For example, an individual's age is typically a number between 0 and 100 while earnings can be number between 0 and 1000000 (or higher). In order to circumvent this problem, we can scale our features to the same range (eg [0,1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model and Model Fitting\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We need to munge our dataset into our **features** (predictors, or independent variables, or $X$ variables) and **labels** (dependent variables, or $Y$ variables).  For ease of reference, in subsequent examples, names of variables that pertain to predictors will start with \"`X_`\", and names of variables that pertain to outcome variables will start with \"`y_`\".\n",
    "\n",
    "But it's not enough to just build the model; we're going to need a way to know whether or not it geenralizes to new data or in to the future. Convincing others of the quality of results is often the *most challenging* part of an analysis.  Making repeatable, well-documented work with clear success metrics makes all the difference.\n",
    "\n",
    "To convince ourselves - and others - that our modeling results will generalize, we need to hold some data back (not using it to train the model), then apply our model to that hold-out set and \"blindly\" predict, comparing the model's predictions to what we actually observed. This is called **cross-validation**, and it's the best way we have to estimate how a model will perform on *entirely* novel data. We call the data used to build the model the **training set**, and the rest the **test set**.\n",
    "\n",
    "In general, we'd like our training set to be as large as possible, to allow our model to be built with as much data as possible. However, you also want to be as confident as possible that your model will generalize to new data. In practice, you'll have to balance these two objectives in a reasonable way.  \n",
    "\n",
    "There are also many ways to split up your data into training and testing sets. Since you're trying to evaluate how your model will perform *in practice*, it's best to emulate the true use case of your model as closely as possible when you decide how to evaluate it. A good [tutorial on cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) can be found on the `scikit-learn` site.\n",
    "\n",
    "One simple and commonly used method is ***k-fold* cross-validation**, which entails splitting up our dataset into *k* groups, holding out one group while training a model on the rest of the data, evaluating model performance on the held-out \"fold,\" and repeating this process *k* times (we'll get back to this in the text-analysis tutorial). Another method is **temporal validation**, which involves building a model using all the data up until a given point in time, and then testing the model on observations that happened after that point. \n",
    "\n",
    "Our current problem is a problem in time where we are trying to predict an event in the future. Generally, if you use the future to predict the past there will be temporal effects that will help the accuracy of your predictions. We cannot use the future to predict the past in real life, so it is important to use `temporal validation` and create our training and test sets accordingly.  \n",
    "\n",
    "*Note: it is important to segregate your data based on time when creating features. Otherwise there can be \"leakage,\" where you accidentally use information that you would not have known at the time.*  This happens often when calculating aggregation features; for instance, it is quite easy to calculate an average using values that go beyond our training set time-span and not realize it.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Sets\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We have a single test sample of data, so we will use the sklearn function `train_test_split` to separate into training and testing datastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pauli\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1658500 entries, 0 to 1658499\n",
      "Columns: 330 entries, PATENT_ID to label\n",
      "dtypes: bool(327), float64(1), object(2)\n",
      "memory usage: 555.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/ML_sample_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATENT_ID</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>106</th>\n",
       "      <th>128</th>\n",
       "      <th>137</th>\n",
       "      <th>165</th>\n",
       "      <th>204</th>\n",
       "      <th>210</th>\n",
       "      <th>23</th>\n",
       "      <th>239</th>\n",
       "      <th>...</th>\n",
       "      <th>Urologic Diseases</th>\n",
       "      <th>Usher Syndrome</th>\n",
       "      <th>Uterine Cancer</th>\n",
       "      <th>Vaccine Related</th>\n",
       "      <th>Vaccine Related (AIDS)</th>\n",
       "      <th>Vascular Cognitive Impairment/Dementia</th>\n",
       "      <th>Vector-Borne Diseases</th>\n",
       "      <th>West Nile Virus</th>\n",
       "      <th>freq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7838231</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8323916</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8642046</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7141367</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7498151</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PATENT_ID   PROJECT_ID    106    128    137    165    204    210     23  \\\n",
       "0   7838231  R01DK068306  False  False  False  False  False  False  False   \n",
       "1   8323916  R01DK068306  False  False  False  False  False  False  False   \n",
       "2   8642046  R01DK068306  False  False  False  False  False  False  False   \n",
       "3   7141367  R01DK068306  False  False  False  False  False  False  False   \n",
       "4   7498151  R01DK068306  False  False  False  False  False  False  False   \n",
       "\n",
       "     239  ...  Urologic Diseases  Usher Syndrome  Uterine Cancer  \\\n",
       "0  False  ...              False           False           False   \n",
       "1  False  ...              False           False           False   \n",
       "2  False  ...              False           False           False   \n",
       "3  False  ...              False           False           False   \n",
       "4  False  ...              False           False           False   \n",
       "\n",
       "   Vaccine Related  Vaccine Related (AIDS)  \\\n",
       "0            False                   False   \n",
       "1            False                   False   \n",
       "2            False                   False   \n",
       "3            False                   False   \n",
       "4            False                   False   \n",
       "\n",
       "   Vascular Cognitive Impairment/Dementia  Vector-Borne Diseases  \\\n",
       "0                                   False                  False   \n",
       "1                                   False                  False   \n",
       "2                                   False                  False   \n",
       "3                                   False                  False   \n",
       "4                                   False                  False   \n",
       "\n",
       "   West Nile Virus  freq  label  \n",
       "0            False   8.0   True  \n",
       "1            False   0.0  False  \n",
       "2            False   0.0  False  \n",
       "3            False   0.0  False  \n",
       "4            False   0.0  False  \n",
       "\n",
       "[5 rows x 330 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra patent type from the PatentView data\n",
    "patent = pd.read_csv('../Data/patent.tsv',sep='\\t',error_bad_lines=False,usecols = ['id','type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATENT_ID</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000000</td>\n",
       "      <td>utility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000001</td>\n",
       "      <td>utility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000002</td>\n",
       "      <td>utility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000003</td>\n",
       "      <td>utility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000004</td>\n",
       "      <td>utility</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PATENT_ID     type\n",
       "0  10000000  utility\n",
       "1  10000001  utility\n",
       "2  10000002  utility\n",
       "3  10000003  utility\n",
       "4  10000004  utility"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent = patent.rename(columns={'id':'PATENT_ID'})\n",
    "patent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7144425 entries, 0 to 7144424\n",
      "Data columns (total 2 columns):\n",
      "PATENT_ID    object\n",
      "type         object\n",
      "dtypes: object(2)\n",
      "memory usage: 109.0+ MB\n"
     ]
    }
   ],
   "source": [
    "patent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge = df.merge(patent,on=['PATENT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72075 entries, 0 to 72074\n",
      "Columns: 331 entries, PATENT_ID to type\n",
      "dtypes: bool(327), float64(1), object(3)\n",
      "memory usage: 25.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dfmerge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATENT_ID</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>106</th>\n",
       "      <th>128</th>\n",
       "      <th>137</th>\n",
       "      <th>165</th>\n",
       "      <th>204</th>\n",
       "      <th>210</th>\n",
       "      <th>23</th>\n",
       "      <th>239</th>\n",
       "      <th>...</th>\n",
       "      <th>Usher Syndrome</th>\n",
       "      <th>Uterine Cancer</th>\n",
       "      <th>Vaccine Related</th>\n",
       "      <th>Vaccine Related (AIDS)</th>\n",
       "      <th>Vascular Cognitive Impairment/Dementia</th>\n",
       "      <th>Vector-Borne Diseases</th>\n",
       "      <th>West Nile Virus</th>\n",
       "      <th>freq</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D309496</td>\n",
       "      <td>R01DK068306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D309496</td>\n",
       "      <td>R01DK059597</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D309496</td>\n",
       "      <td>R01DK058816</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D309496</td>\n",
       "      <td>R01AI055058</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D309496</td>\n",
       "      <td>P30ES005605</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>design</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PATENT_ID   PROJECT_ID    106    128    137    165    204    210     23  \\\n",
       "0   D309496  R01DK068306  False  False  False  False  False  False  False   \n",
       "1   D309496  R01DK059597  False  False  False  False  False  False  False   \n",
       "2   D309496  R01DK058816  False  False  False  False  False  False  False   \n",
       "3   D309496  R01AI055058  False  False  False  False  False  False  False   \n",
       "4   D309496  P30ES005605  False  False  False  False  False  False  False   \n",
       "\n",
       "     239  ...  Usher Syndrome  Uterine Cancer  Vaccine Related  \\\n",
       "0  False  ...           False           False            False   \n",
       "1  False  ...           False           False            False   \n",
       "2  False  ...           False           False            False   \n",
       "3  False  ...           False           False             True   \n",
       "4  False  ...           False           False            False   \n",
       "\n",
       "   Vaccine Related (AIDS)  Vascular Cognitive Impairment/Dementia  \\\n",
       "0                   False                                   False   \n",
       "1                   False                                   False   \n",
       "2                   False                                   False   \n",
       "3                   False                                   False   \n",
       "4                   False                                   False   \n",
       "\n",
       "   Vector-Borne Diseases  West Nile Virus  freq  label    type  \n",
       "0                  False            False   0.0  False  design  \n",
       "1                  False            False   0.0  False  design  \n",
       "2                  False            False   0.0  False  design  \n",
       "3                  False            False   0.0  False  design  \n",
       "4                  False            False   0.0  False  design  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['design', 'utility'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmerge['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge = pd.get_dummies(dfmerge, columns=['type'], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['106', '128', '137', '165', '204', '210', '23', '239', '244', '250', '252', '257', '260', '264', '273', '280', '29', '313', '324', '335', '343', '345', '348', '349', '351', '356', '359', '361', '364', '367', '372', '378', '382', '384', '385', '395', '403', '406', '415', '417', '420', '422', '423', '424', '425', '426', '427', '428', '433', '434', '435', '436', '5', '502', '504', '506', '510', '514', '520', '521', '522', '523', '524', '525', '527', '528', '530', '534', '536', '540', '544', '546', '548', '549', '550', '552', '554', '556', '558', '560', '562', '564', '568', '574', '585', '600', '601', '602', '604', '606', '607', '611', '62', '623', '675', '7', '702', '703', '704', '705', '706', '707', '709', '712', '715', '717', '73', '800', '850', '860', '930', '935', '96', '977', 'D23', 'ALS', 'Acquired Cognitive Impairment', 'Acute Respiratory Distress Syndrome', 'Adolescent Sexual Activity', 'Aging', 'Alcoholism, Alcohol Use and Health', \"Alzheimer's Disease\", \"Alzheimer's Disease Related Dementias (ADRD)\", \"Alzheimer's Disease including Alzheimer's Disease Related Dementias (AD/ADRD)\", 'American Indian or Alaska Native', 'Anthrax', 'Antimicrobial Resistance', 'Anxiety Disorders', 'Arthritis', 'Assistive Technology', 'Asthma', 'Atherosclerosis', 'Attention Deficit Disorder (ADD)', 'Autism', 'Autoimmune Disease', 'Basic Behavioral and Social Science', 'Batten Disease', 'Behavioral and Social Science', 'Biodefense', 'Bioengineering', 'Biomedical Imaging', 'Brain Cancer', 'Brain Disorders', 'Breast Cancer', 'Breastfeeding, Lactation and Breast Milk', 'Burden of Illness', 'Cancer Genomics', 'Cannabinoid Research', 'Cardiovascular', 'Caregiving Research', 'Cerebrovascular', 'Cervical Cancer', 'Childhood Leukemia', 'Childhood Obesity', 'Chronic Liver Disease and Cirrhosis', 'Chronic Pain', 'Climate Change', 'Climate-Related Exposures and Conditions', 'Clinical Trials and Supportive Activities', 'Colo-Rectal Cancer', 'Comparative Effectiveness Research', 'Complementary and Alternative Medicine', 'Conditions Affecting the Embryonic and Fetal Periods', 'Congenital Structural Anomalies', 'Contraception/Reproduction', \"Cooley's Anemia\", 'Cost Effectiveness Research', \"Crohn's Disease\", 'Cystic Fibrosis', 'Dementia', 'Dental/Oral and Craniofacial Disease', 'Depression', 'Diabetes', 'Dietary Supplements', 'Digestive Diseases', 'Down Syndrome', 'Drug Abuse (NIDA only)', 'Duchenne/ Becker Muscular Dystrophy', 'Dystonia', 'Eczema / Atopic Dermatitis', 'Emerging Infectious Diseases', 'Endocannabinoid System Research', 'Endocrine Disruptors', 'Epilepsy', 'Estrogen', 'Eye Disease and Disorders of Vision', 'Facioscapulohumeral Muscular Dystrophy', 'Fetal Alcohol Syndrome', 'Food Allergies', 'Foodborne Illness', 'Frontotemporal Dementia (FTD)', 'Gene Therapy', 'Gene Therapy Clinical Trials', 'Genetic Testing', 'HIV/AIDS', 'HPV and/or Cervical Cancer Vaccines', 'Headaches', 'Health Effects of Indoor Air Pollution', 'Health Services', 'Heart Disease', 'Heart Disease - Coronary Heart Disease', 'Hematology', 'Hepatitis', 'Hepatitis - B', 'Hepatitis - C', \"Hodgkin's Disease\", 'Human Fetal Tissue', 'Human Genome', \"Huntington's Disease\", 'Hypertension', 'Immunization', 'Infant Mortality', 'Infectious Diseases', 'Infertility', 'Inflammatory Bowel Disease', 'Influenza', 'Injury (total) Accidents/Adverse Effects', 'Injury - Childhood Injuries', 'Injury - Trauma - (Head and Spine)', 'Injury - Traumatic brain injury', 'Intellectual and Developmental Disabilities (IDD)', 'Kidney Disease', 'Lewy Body Dementia', 'Liver Cancer', 'Liver Disease', 'Lung', 'Lung Cancer', 'Lupus', 'Lyme Disease', 'Lymphoma', 'Macular Degeneration', 'Major Depressive Disorder', 'Malaria', 'Malaria Vaccine', 'Maternal Health', 'Mental Illness', 'Methamphetamine', 'Migraines', 'Mind and Body', 'Multiple Sclerosis', 'Muscular Dystrophy', 'Myotonic Dystrophy', 'Nanotechnology', 'Neonatal Respiratory Distress', 'Networking and Information Technology R and D', 'Neuroblastoma', 'Neurodegenerative', 'Neurofibromatosis', 'Neurosciences', 'Nutrition', 'Obesity', 'Opioid Misuse and Addiction', 'Opioids', 'Organ Transplantation', 'Orphan Drug', 'Osteoarthritis', 'Osteoporosis', 'Otitis Media', 'Ovarian Cancer', 'Pain Research', 'Pancreatic Cancer', \"Parkinson's Disease\", 'Patient Safety', 'Pediatric', 'Pediatric AIDS', 'Pediatric Cancer', 'Pediatric Cardiomyopathy', 'Pediatric Research Initiative', 'Perinatal Period - Conditions Originating in Perinatal Period', 'Peripheral Neuropathy', 'Physical Activity', 'Physical Rehabilitation', \"Pick's Disease\", 'Pneumonia', 'Pneumonia and Influenza', 'Polycystic Kidney Disease', 'Post-Traumatic Stress Disorder (PTSD)', 'Precision Medicine', 'Pregnancy', 'Prescription Drug Abuse', 'Preterm, Low Birth Weight and Health of the Newborn', 'Prevention', 'Prostate Cancer', 'Radiation Oncology', 'Rare Diseases', 'Regenerative Medicine', 'Rehabilitation', 'Rheumatoid Arthritis', 'Rural Health', 'Schizophrenia', 'Sepsis', 'Serious Mental Illness', 'Sexual and Gender Minorities (SGM/LGBT*)', 'Sexually Transmitted Infections', 'Sickle Cell Disease', 'Sleep Research', 'Spinal Muscular Atrophy', 'Stem Cell Research', 'Stem Cell Research - Embryonic - Human', 'Stem Cell Research - Embryonic - Non-Human', 'Stem Cell Research - Induced Pluripotent Stem Cell', 'Stem Cell Research - Induced Pluripotent Stem Cell - Human', 'Stem Cell Research - Induced Pluripotent Stem Cell - Non-Human', 'Stem Cell Research - Nonembryonic - Human', 'Stem Cell Research - Nonembryonic - Non-Human', 'Stem Cell Research - Umbilical Cord Blood/ Placenta', 'Stem Cell Research - Umbilical Cord Blood/ Placenta - Human', 'Stroke', 'Substance Abuse', 'Therapeutic Cannabinoid Research', 'Tobacco', 'Tobacco Smoke and Health', 'Transmissible Spongiform Encephalopathy (TSE)', 'Transplantation', 'Tuberculosis', 'Tuberculosis Vaccine', 'Underage Drinking', 'Underage Drinking - Prevention and Treatment (NIAAA Only)', 'Urologic Diseases', 'Usher Syndrome', 'Uterine Cancer', 'Vaccine Related', 'Vaccine Related (AIDS)', 'Vascular Cognitive Impairment/Dementia', 'Vector-Borne Diseases', 'West Nile Virus', 'type_design', 'type_utility']\n"
     ]
    }
   ],
   "source": [
    "# before we split, let's make sure we know what our features are (ie remove IDs)\n",
    "sel_features = dfmerge.columns.tolist()\n",
    "\n",
    "sel_features.remove('PATENT_ID')\n",
    "sel_features.remove('PROJECT_ID')\n",
    "sel_features.remove('freq')\n",
    "sel_features.remove('label')\n",
    "\n",
    "print(sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    71972\n",
       "True       103\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmerge['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.998571\n",
       "True     0.001429\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmerge['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm there are no NULL values in our input data\n",
    "dfmerge.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split the dataframe into training and testing sets for our X and Y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfmerge[sel_features], dfmerge['label'], test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Understanding and Evaluation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this phase, we will run the machine learning model on our training set. The training set's features will be used to predict the labels. Once our model is created using the training set, we will assess its quality by applying it to the test set and by comparing the *predicted values* to the *actual values* for each record in your testing data set. \n",
    "\n",
    "- **Performance Estimation**: How well will our model do once it is deployed and applied to new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Machine Learning Model\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python's [`scikit-learn`](http://scikit-learn.org/stable/) is a commonly-used, well-documented Python library for machine learning. This library can help you split your data into training and test sets, fit models and use them to predict results on new data, and evaluate your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Let's fit a model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score between 0 and 1 (that can sometimes be interpreted as a probability), which is basically the model ranking all of the observations from *most likely* to *least likely* to have label of 1. The 0-1 score is then turned into a 0 or 1 based on a threshold. \n",
    "\n",
    "If you use the sklearn method `.predict()` then the model will select a threshold for you (generally 0.5) - it is almost **never a good idea to let the model choose the threshold for you**. Instead, you should get the actual score and test different threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction scores\n",
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27b9bf3b5f8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAERCAYAAAAAMhLvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZ10lEQVR4nO3de5RlZXnn8W8HWpo7RPCCQkBhPbZGQRAR78pCmWG01RE1jniLmox4Q8EYHTAqGhVQiDgxGBMjxktEcElEiAOSQURQIRi0fRCUSzByGy5Nc+/u+eN9j7U9nDrnVFdV1yne72ct1j679vu8e1ez1/nVvr17ybp165Ak6YHu9xZ6AyRJ2hAMPElSEww8SVITDDxJUhMMPElSEzZe6A1QEREXA7sAtwOXL/DmSNJisSuwBfCrzHzisIYG3uTYBdi6/veIBd4WSVpsdhnVwMCbHLcDW2+55ZYsX758obdFkhaFlStXsmrVKijfoUMZeJPjcuARy5cv56STTlrobZGkReHggw/mwgsvhDEuBXnTiiSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCT6H9wBxwS9vGvjzfR714A28JZI0mQy8B5Arblj9O/OP3n7zBdoSSZo8ntKUJDXBwJMkNcHAkyQ1wcCTJDXBwJMkNcHAkyQ1wcCTJDXBwJMkNcHAkyQ1wcCTJDXBwJMkNcHAkyQ1wcCTJDXBwJMkNcHAkyQ1wcCTJDXBwJMkNcHAkyQ1wcCTJDVh45kWRMR/A/4YeDKwPXAHcBlwKvDpzLxtmrqtgMOA/w48CrgbSOALwN9k5n1D1vl7wOuA1wOPr9t9FfAN4JjMvGnENj8HOBTYF9gKuA44Czg2My8dUbsr8B5gf+DhwM3Aj4ETMvP0YbWSpMkx9hFeRGwcEV8GTgNeBOwALAW2BvYGPgL8W0QsH1D7EOBHwBHAY4Flte7JwAnA/42ILadZ70bAycDfAk8FtgQ2BR5DCaJ/j4jHD9nudwJnAy8AtgMeBOwIvBb4UUS8ckjt04FLKAG/U/19HwL8F+BbEfFX09VKkibLTE5pfhx4Rf38FWAfSoA8EfgQcA+wCyUINu8V1aOzbwG7UY6O3kA5Unp0rbuPcuT1uWnWezTwYmAtJVR3BR5GOeK7qfb1ze46O+teARxTZ08F9qIcle5PCbJNgL+PiCcOqP0D4JvAZsBPgefX2j2Br9Vmb42IN0+z3ZKkCTLWKc2IeATw1jp7Ymb+SWfxTZQju/OB0ymh9z+ZCpqXAU+qn1dk5rmd2iMj4lrgM8BBEfGUzPxBZ707Am+ps+/PzKM6tZ+PiIuAC4Gd6/Z9tFO7hBKQS4DvAC/NzLV18f+JiGfW2gD+Ejig79f+c2BbyunPZ2fmjfXnNwIvi4gvAX8EfCAivpCZtw/4p5MkTYhxj/BWUMJxHXDkoAaZ+W3g/Dp7YGfRoXV6Zl/Y9ZwI/Lx+fkPfsrdQTiPeDBw7YJ0/AT4/Te0BlNOnAEd2wq5XexvwF3X2eTVcAYiIbSinPAGO64Rd1+HAGspR7osGLJckTZBxA28H4C7gqsy8bki7yzvtiYgHU67vQbn2dz+ZuQ745zr7wr7FvaOuszLzzmnW2ev30RHxuAG1NwAXTFN7OiW0lvSt+zmU053Dtvta4OI6u2Ka/iVJE2KswMvM/5WZmwK7j2j66Dq9uU6fQAkTgIuG1PWCY/uI2AkgIh7E1BHaOLVQrtH17NFbXkP1fupR3i+H1N4N/GyMde81pI0kaQLM6Dm86R45AKh3Su5bZ79Xpzt3mlw5pOurO593qdNHMHWNcVjtb4B7+2q76x5W2133oNqrpgvLvtqd6s05kqQJNSdf0vVo7ETK0dx99TOU61s9twzp4tbO521nUluvza3qq+3WD1tvd92zqd2I8nyfJGlCzTrw6pHN54Cn1B8dk5mX1c/LOk3vGtJN9/rcsr5p//Jh9d2aZX3L5rO2v16SNGFmFXj1ofDPAa+qP/ou5eHynjV1Ouy0IExd5+ta0/k8m/oNXStJmkDrHXgRsRnwdaZu378AeFHfEGGr63QJU3c9DjLoaG5152ebjticQUdkvfoNUdtfL0maMOsVeBHxUOAcpm7HPwfYf8BNLd1rYFsP6XKbzufeM29j1dZTqr1hybrPy/Xqh623u+7Z1N4LTHtDjyRp4c048CIiKA+Y956vOxk4IDNXDWh+WefzTkO63bHzuXfn4zVMHTUNq30Y5eH0bm133cNqu+ueTe01I+7mlCQtsBkFXkTsDnyfqVv4PwG8PDPvnqbkp0xdB9tjmjZQxqcEuCkzr4Hf3n25cga18LvP5PXegjBtbX2Dw6OG1G4WEbuNse6Lh7SRJE2AmbwtYTfKmJS/Twmxd2Tmu/qH7OqqpzjPq7P9o6j0+l3C1FBkZ/Qt7r1+Z/+ImO4uyBfU6X9QAra/doeImO7B8AMpjxQAnNn5+XeZOrqcbrsfSRk4e9B2S5ImzFiBV5+z+wrlbQEAb8rM48dcxxfq9MCI2G/A8jdRXvUDcFzfsn+kvCVhe+C9A7brCcBr6uzxfacVz6GcFgU4OiI27qvdCnh/nf1WZv6it6wOBH1qnT08Ih4+YLs/Tvn3u6FupyRpgo37Atg3MXX67h+Ar0TEFkPar83MO+rnvwMOoQxL9o2I+HPgFMp1t9cB76vtTs7MH3U7ycyfR8RngDcDR0TE1sCnKTeVHEAZUHoT4FfA3/TVromIdwH/RBkb8/SIOIIy3ufutTYow4e9n/t7L2VQ6IcC50bEoZRrlzvUbX5ZbfeBIeN8SpImxLinNN/R+fwaysgmw/777fiTmbkGeAnlDeVbAJ8CrqUM+fV+Suiez9SRWr/DKKdSAd5GeUv6dZTg3Y5yhDXwppnM/BrQe6XQ/sAPKHdjnkW5tncf8KrM/PGA2qsooXYXZYzQb9Z1XcJU2B2fmZ+eZrslSRNkZOBFxHZMDQq9XjLzl5SBpD9Euc52B+Ua2cWU1+w8u3NE2F97J+Vo7o2U64G3Ul42ewVwPPCEzsgug+qPAPajBNb1lJD7DfBVYJ/MPHlI7beAP6S8bf2qut5bKW9Qf2lmvmO6WknSZBl5SrO+C27WI4rUG1iOZJr36Y2oXUsJnb9dz3WfTQmp9am9ghK2kqRFzBH+JUlNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU0w8CRJTTDwJElNMPAkSU3YeLYdRMQ3gBXAMzLze0PaPQx4D3AgsBOwCrgU+Gxm/uOIdTwIeCvwSuAxwFrgl8BXgeMy844R9S8B3gzsBWwKXAucDhyTmVeNqN0TOBx4FrAdcANwXl3v94fVSpImx6yO8CLiEErYjWoXwE+AtwO7Ag8CHkwJkS9GxCkRMTB8I2Iz4BzgGGBPYDNgC+AJwIeBiyJihyHrPg74OrAfsA2wCfAo4C3AJRHx3CG1BwEXAK8AHg4sBXYADgLOjYjDR/3ukqTJsN6BFxFvAD41RrutgDOA7YFrgJcCDwEeC/zv2uzFwEem6eKLwL7AXcBhlKPDHYFDgTuAAE6NiCUD1v02SsgCnAg8rq77JcCVwNbA1yPiEQNq9wZOohwFnws8vf4OTwO+S/m3+1hEHDjq30CStPBmfEqznl78BHDImCVvBnYG7gaem5mX15/fABwSEbdRTnW+PSJOyMyrO+vahxKGAG/MzC92+j0uIlZSwvTJwMuBr3RqNweOrLOfy8w/6dSeGhE/BC6ihNgRwJ/2bfeHKEeDPwWel5l31Z/fGBHPB84CngEcHRHfzsy1Y/57SJIWwIyO8CLixZQA6IXdj0e0XwK8rc5+vhN2XR8Cbqac5nx137J31unPgftd58vMM4Ez6+wb+ha/mnLadC1Twdet/Q9KcAP8j4hY1tnu5cDz6+xRnbDr1d4LvLvOLgeeOuD3kiRNkLEDLyK2AU6hXINbTbkGdtiIsj0o174AThvUoN5wclad/e31wBqWvdD558xcN806ev0+OyK27vz8gDq9KDN/PaJ2C8o1vv7aNcC3p6m9ALi+f7slSZNpptfw1lCuaz02Mz89Rvs9Op8vGtLu4jrdvXPzys6Ua2zj1m4E7D5g3cNqfwbcUz/vNaD2isy8dVBhDeBLBtRKkibQTAJvNbBrZr66e51thJ3r9G7gN0Pa9fpbCvRuINm5s/zKMWoBdgGIiI2AR46qraF1Tbe2b93D1ttd9y5DW0mSFtzYN63U61ZXzrD/7er01iGnJAG6R1HbAld1agFumUEtwO8zFebDarv123Z+1lv3+tRKkibQfI+00rsR5M4R7brLl/VNR9XPpra7vFsz0+1eNrSVJGnBzXfgranTYUd3APd7hq5TO079bGqHrXt9aiVJE2i+A291nW46ot2gI7LVnZ8Nq+8um2ltd93do7mZbveoI0FJ0gKb78DrXQPbakS7bTqfb+yrham7NcetXcXUUdqw2m79jZ2f9da9PrWSpAk034F3WZ1uGhHbD2m3Y53eA1zXVwtlOLFRtVDvmqyjnlwxqrY+69e7m7N7t2dv3cPW2133uHetSpIWyHwH3qWdz3tM26oMCg3w75l5H0Bm/idw0wxq11IGqO5f97Dax1FGeIGp5/m6tbvVIcrup4Zlr++LB7WRJE2ODRF4vefcXjioQX0bQu+NBWf0Le6NcvKCIevoLTu/7yHx0+t07/pqomG1dwL/OqB2KVOjrvR7CmUcTrj/dkuSJsy8Bl599u6kOvvHEfG4Ac2OoDzHdg/w133LvlCnj4+I1/cX1kGcn1dnj+tbfApwOyW0Pjag9pGUNy4A/F1mrups9xVA7113H4yILfpqlwIfrbOXMjU0miRpQs36BbBj+BjwOsqYmmdHxLuAf6Hc8PE2pgai/qvMvLZbmJnfiYjTgf8KfKaG1D8A91JeM9R7pdCFlIDr1t4cER8EPg68uhNSv6a84ueTlCO0/8eAQKQMXH0+5TVG59R33/0E2I3yHr5nUh5beO+Ih+olSRNgvk9pkpm3UQZXvpnyLrqTKDemJFNhdwrwZ9N08Wrg3yhHah+gjPZyLXA8sDnwC+AF07ye51hKQAL8EWXsyxuAb1CGA7sDeGFmXtNfmJkXUF4ZtJYyVubZlLsxz2fqFOy7MnPgoNiSpMky74EHkJk/pLxG5zjgcsrYmrdTThu+EThouvfJZeZNlOtlh1FeR3R7rV8JHAU8KTOvn6Z2bWa+lnI0+B3K0dx9lOuKnwN2z8zzhmz3icDewJcpR4b3Um6kOQ3YLzM/OfY/giRpQc3qlGZmnsOYo41k5nWUa2aHjmo7oPZuytHasTOtrfVfB76+nrUXAa9cn1pJ0uTYIEd4kiQtNANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUhI0XegMmXUTsCRwOPAvYDrgBOA84LjO/v5DbJkkan0d4Q0TEQcAFwCuAhwNLgR2Ag4BzI+LwBdw8SdIMGHjTiIi9gZMoR8HnAk8HtgeeBnyX8m/3sYg4cME2UpI0NgNveh8CNgF+CjwvM8/LzBvracznU0JwCXB0RPjvKEkTzi/qASJiOSXUAI7KzLu6yzPzXuDddXY58NQNuHmSpPVg4A12QJ2uAb49TZsLgOvr5xXzvkWSpFkx8Abbo06vyMxbBzXIzHXAJXV2rw2yVZKk9WbgDbZznV45ot3VdbrLvG3JLGy/5SYLvQmSNDF8Dm+w7er0lhHtekd/287BOncFWLlyJQcffPCMi2+78977/eyUjX6Pe9esHdh+q02XzngdkjRpVq5c2fu466i2Bt5gy+r0zhHtesuXDW01ni0AVq1axYUXXjgH3UlSU7YY1cDAG2xNna4b0W7JHK7zV5RTo7cDl89hv5L0QLYrJex+NaqhgTfY6jrddES7cY8ER8rMJ862D0nS9LxpZbDetbutR7Tbpk5vnMdtkSTNAQNvsMvqdKcR7Xas06uHtpIkLTgDb7BL63S3iNh8UIOIWMLU83oXb5CtkiStNwNvsNPrdClTo670ewplMGmAM+Z9iyRJs2LgDZCZVwC9d919MCJ+53bXiFgKfLTOXgqctQE3T5K0HpasWzfqzvs2RcQ+wPmURw9+THkJ7E+A3YAPA8+lPLawIjNPW6jtlCSNx8AbIiLeBPw10x8JvzMzP7kBN0mStJ4MvBEiYk/gMOBZlGt2t1FOdx6XmWcv5LZJksZn4EmSmuBNK5KkJhh4kqQmGHiSpCYYeJKkJvi2hAlS7wg9nHJH6HbADcB5lDtCvz+sdkS/zwEOBfYFtgKuozwsf2xmXjqsVg8c87F/RcS/As8co+nj3dfaFBHfAFYAz8jM782in1l/j3mX5oSIiIOALzH4j5C1wHsy8+j16PedwLHTLL4beH1mfmmm/WpxmY/9q44newvly2cUA69BEXEIcEKdXe/Am6vvMU9pToCI2Bs4ifJldC7wdMozf08Dvkv5//SxiDhwhv2uAI6ps6cCe9V+9wcuATYB/j4ifBffA9h87V+UF2/2wm5fYMsh//1sdr+FFpuIeAPwqTnoZ86+xzzCmwARcQbwfOCnwJMy867OsqWUw/ZnACuBP8zMtWP0uYQyzudjge8AB3TrImIr4EIggDMzc7pBsrXIzcf+VWtfDnyF8gLkrTLzvrnedi0+EfEg4BPAIX2LZnyEN9ffYx7hLbCIWE75MgI4qvtlBJCZ9wLvrrPLgaeO2fUBlJ0E4Mj+L7HMvA34izr7vIjYET3gzOP+BbBnnV5s2AkgIl5M+cOqF3Y/nmWXc/o9ZuAtvN5fJGuAb0/T5gLg+vp5xQz7vaHWD3J6Xe8S4IVj9qvFZb72L5gKvB+ux3bpASYitgFOoZzqXg28hTIs42zM6feYgbfwei+RvSIzbx3UIDPXUc5VQzl/PZN+L671g/q9DfjlDPvV4jJf+xdA75rJZRHxvoi4OCLuiIhVEXFhRLy9nt5SO9ZQrhc/NjM/PQf9zen3mI8lLLyd6/TKEe2urtNd5qHf3WbQrxaXnev0yhHtZrR/RcQfAA+us58E+oNt7/rfayLiwMz8z3H61aK2Gtg1M6+cwz53rtNRfY71PeYR3sLbrk5vGdGu99f5tgvcrxaX+doP9ux8Xku5jvKYur6nAifXZU8EvhkRm4zZrxapzLx3jsMO5nj/9Qhv4S2r0ztHtOstXza01fz3q8VlvvaD7SjX/bYCnpOZP+gsOx84KCKOB94GPAl4E3Nwi7qaM6f7r0d4C29NnY56PmTJhPSrxWVe9oPM/GxmPhTYui/suv6McrMBwOtm0r9Uzen+a+AtvNV1uumIduP+pTPf/Wpxmdf9IDPvGbLsLuBf6uwentbUepjT/dfAW3i9c9Nbj2i3TZ3euMD9anFZ6P2gdzPMEqZucpHGNaf7r4G38C6r051GtOs9UHn10Fbz368Wl3ndD+pIGMN07968YyZ9S8zx/mvgLbzegLq7RcTmgxrUL5XfPo8yw373mK5BHZbnUTPsV4vLvOxfEXFBRNxCGdtwmN4oGddn5qg77aR+c/o9ZuAtvNPrdClTowr0ewplsFSAM2bY7w4RMd3DmAcCG9XPZ47ZrxaX+dq/bqecZnpORAy8My4iHgY8d4b9Sl1z+j1m4C2wzLwC6L2L7IMRsUV3eR3c96N19lLKQL/jOAe4pn4+OiJ+5xGU+lfR++vstzLzFzPcdC0C87h/9V7FshXw4f6FdX/7LGUk+7XAcTPbcgmY4+8x35YwASJiH8qzS0sog60eDvyEMnLAhyl/Ja8DVmTmaZ26JwNfqLMnZOYJff0eBPxTnf0OcARwObA75d1Se1DeJfW0zJztIK+aUPOxf9Uvnh8wNZTT5ynvPbsKeBzlQfRn12Ufycz3zf1vpkkXEc+mvIIKpnlbwob8HvMIbwJk5gXAn1L+Et4LOJtyt9H5TJ0Self3y6jajPJajGBqRIJuv18Djqqz+1O+oG6k/BW/B3Af8CrD7oFtPvav+naEFzB1zeS1wI8oz92dQwm7dZS3qRt2GmaDfY8ZeBMiM0+kjD34ZeDXwL3ATcBpwH6Z+cn17PcIYD/gm5SRMe4DfgN8FdgnM08eUq4HiPnYv+r4mPsCb6a8WPYW4B7KKagvA8/KzEPn5BdQ0+bqe8xTmpKkJniEJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJasL/Bza6MfPSRyL/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(X_test)\n",
    "\n",
    "df_eval['label'] = y_test.astype(int)\n",
    "\n",
    "df_eval['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>y_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23785.000000</td>\n",
       "      <td>23785.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.040461</td>\n",
       "      <td>0.023667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label       y_score\n",
       "count  23785.000000  23785.000000\n",
       "mean       0.001640      0.001588\n",
       "std        0.040461      0.023667\n",
       "min        0.000000      0.000000\n",
       "25%        0.000000      0.000000\n",
       "50%        0.000000      0.000000\n",
       "75%        0.000000      0.000000\n",
       "max        1.000000      1.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[['label', 'y_score']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. What threshold would you choose given the resulting \"scores\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make a simple function in one line using \"lambda\":\n",
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "\n",
    "# given the distribution of the scores, what threshold would you set?\n",
    "selected_threshold = 0.02\n",
    "\n",
    "# create a list of our predicted outocmes\n",
    "predicted = np.array( [calc_threshold(score, selected_threshold) for score in y_scores] )\n",
    "\n",
    "# and our actual, or expected, outcomes\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23474   272]\n",
      " [   31     8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9872608787050662\n"
     ]
    }
   ],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.99836\n",
       "1    0.00164\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we think about this accuracy? Good? Bad?\n",
    "\n",
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics that are often more relevant than overall accuracy are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds, we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall at k%\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve, we can focus on more fine-grained metrics. For instance, say there is a special program for those most likely to need assistance within the next year, but that it can only cover *1% of our test set*. In that case, we would want to prioritize the 1% who are *most likely* to need assistance within the next year and it wouldn't matter too much how accurate we were on the overall data.\n",
    "\n",
    "Let's say that, out of the approximately 300,000 observations, we can intervene on 1% of them, or the \"top\" 3,000 in a year (where \"top\" means highest likelihood of needing intervention in the next year). We can then focus on optimizing our **precision at 1%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i > threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.3f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Understanding\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have evaluated our model overall, let's look at the \"importances\" for each feature, along with their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sel_features, model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Against Baselines\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. \n",
    "\n",
    "> Without any context, over 85% accuracy can sound great... But it's not so great when you remember that you could as well or better by declaring that all firms will survive in the next year, which would be a stupid (not to mention useless) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_p_at_selected = precision_at_k(expected,random_score, 0.01)\n",
    "print(random_p_at_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work is reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or a member of an agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Algorithms\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We have only scratched the surface of what we can do with our model. We've only tried one classifier (Logistic Regression), and there are plenty more classification algorithms in `sklearn`. Let's try them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=100, n_jobs=-1),\n",
    "        'LR': LogisticRegression(),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17\n",
    "                                         , n_estimators=5),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(max_depth=10, min_samples_split=10)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB', 'DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "df_results = pd.DataFrame()\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print(clf)\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    p_at_5 = precision_at_k(expected,y_score,0.05)\n",
    "    p_at_10 = precision_at_k(expected,y_score,0.10)\n",
    "    fpr, tpr, thresholds = roc_curve(expected,y_score)\n",
    "    auc_val = auc(fpr,tpr)\n",
    "    df_results = df_results.append([{\n",
    "        'clfNM':clfNM,\n",
    "        'p_at_1':p_at_1,\n",
    "        'p_at_5':p_at_5,\n",
    "        'p_at_10':p_at_10,\n",
    "        'auc':auc_val,\n",
    "        'clf': clf\n",
    "    }])\n",
    "    \n",
    "    #feature importances\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        feature_import = dict(\n",
    "            zip(sel_features, clf.coef_.ravel()))\n",
    "    elif hasattr(clf, 'feature_importances_'):\n",
    "        feature_import = dict(\n",
    "            zip(sel_features, clf.feature_importances_))\n",
    "    print(\"FEATURE IMPORTANCES\")\n",
    "    print(feature_import)\n",
    "    \n",
    "    if max_p_at_k < p_at_1:\n",
    "        max_p_at_k = p_at_1\n",
    "    print('Precision at 1%: {:.2f}'.format(p_at_1))\n",
    "# df_results.to_csv('output/modelrun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and print out the results of all the above models\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus code: visualizing Decision tree models\n",
    "\n",
    "Note the additional packages required for running the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# packages to display a tree in Jupyter notebooks\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz as gv\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=3, min_samples_split=100)\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the tree\n",
    "\n",
    "# object to hold the graphviz data\n",
    "dot_data = StringIO()\n",
    "\n",
    "# create the visualization\n",
    "export_graphviz(model, out_file=dot_data, filled=True,\n",
    "               rounded=True, special_characters=True,\n",
    "               feature_names=df_training[sel_features].columns.values)\n",
    "\n",
    "# convert to a graph from the data\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the graph to zoom in \n",
    "# graph.write_pdf('./output/model_eval_tree1.pdf')\n",
    "\n",
    "# or view it directly in notebook\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction scores\n",
    "y_scores = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "fig, ax  = subplots(figsize=(10,5))\n",
    "sns.distplot(y_scores, kde=False, rug=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the distribution of the scores, what threshold would you set?\n",
    "selected_threshold = 0.3\n",
    "\n",
    "# create a list of our predicted outocmes\n",
    "predicted = np.array( [calc_threshold(score, selected_threshold) for score in y_scores] )\n",
    "\n",
    "# and our actual, or expected, outcomes\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( model_precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'DTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decistion Trees have a `.feature_importances_` rather than `.coef_` attribute:\n",
    "list(zip(sel_features, model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Our model has just scratched the surface. Try the following: \n",
    "    \n",
    "- Create more features\n",
    "- Try more models\n",
    "- Try different parameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "605px",
    "left": "0px",
    "right": "1492px",
    "top": "110px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
